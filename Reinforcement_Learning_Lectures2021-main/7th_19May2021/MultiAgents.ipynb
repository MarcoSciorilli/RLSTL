{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi agents in Matrix-Games\n",
    "\n",
    "One example of multi-agents Reinforcement Learning, important because of its connections to Game Theory, is that of Agents interacting in Matrix Games.\n",
    "\n",
    "A Matrix Game is, for each player, similar to a multi-armed bandit in the sense that:\n",
    "\n",
    "- There is just one state.\n",
    "- There are multiple choices of actions.\n",
    "\n",
    "What is different is that the reward (*payoff*) for a given action depends also on the actions of the other player. In fact it is what is called an Adversarial bandits problem.\n",
    "\n",
    "\n",
    "#### $2$-players Matrix Games\n",
    "\n",
    "Let us consider $2$-players Matrix Games with 2 actions. At each time $t$ the players simultaneously choose their actions $a_1, a_2$. \n",
    "The payoffs depend on both actions, and can be written as matrices; let us call $A$ the matrix of payoffs for player $1$ and $B$ the matrix of payoffs for player $2$. \n",
    "\n",
    "\n",
    "For player $1$:\n",
    "$$\n",
    "A = \n",
    "\\begin{bmatrix} \n",
    "A_{1 1} & A_{1 2} \\\\\n",
    "A_{2 1} & A_{2 2}\\\\\n",
    "\\end{bmatrix} \n",
    "$$\n",
    "\n",
    "For player $2$:\n",
    "$$\n",
    "B = \n",
    "\\begin{bmatrix} \n",
    "B_{1 1} & B_{1 2} \\\\\n",
    "B_{2 1} & B_{2 2}\\\\\n",
    "\\end{bmatrix} \n",
    "$$\n",
    "\n",
    "This means that if player $1$ acted with action $i$ and player $2$ acted with action $j$, they respective payoffs are $A_{i j}$ and $B_{i j}$. A conventional, condensed way to write them is:\n",
    "\n",
    "$$\n",
    "R = \\begin{bmatrix} \n",
    "A_{1 1}, B_{1 1} & A_{1 2}, B_{1 2} \\\\\n",
    "A_{2 1}, B_{2 1} & A_{2 2}, B_{2 2}\\\\\n",
    "\\end{bmatrix} \n",
    "$$\n",
    "\n",
    "\n",
    "On average, given the policies \n",
    "- $\\pi^{(1)}(1)$,    $\\pi^{(1)}(2) = 1-\\pi^{(1)}(1)$ \n",
    "- $\\pi^{(2)}(1)$,    $\\pi^{(2)}(2) = 1-\\pi^{(2)}(1)$ \n",
    "\n",
    "the expected payoffs are:\n",
    "\n",
    "$$\n",
    "G^{(1)} = \\sum_{i j} A_{i j} \\pi^{(1)}(i) \\pi^{(2)}(j)\\\\\n",
    "G^{(2)} = \\sum_{i j} B_{i j} \\pi^{(1)}(i) \\pi^{(2)}(j) \n",
    "$$\n",
    "\n",
    "These expected payoffs are those we want to maximize.\n",
    "Clearly, now there is no simple answer to the question of what are optimal policies: They depend on what the other agent does! \n",
    "\n",
    "Some games are trivial, in the sense that both agents \"*want the same thing*\". However the most interesting games are those where the two agents have conflicting aims. An important class of games is that of zero-sum reward games: $A_{i j} + B_{ij} = 0$.\n",
    "\n",
    "Consider for example the case of the game of \"matching pennies\".  For player $1$:\n",
    "$$\n",
    "A = \n",
    "\\begin{bmatrix} \n",
    "1 & -1 \\\\\n",
    "-1 & 1\\\\\n",
    "\\end{bmatrix} \n",
    "$$\n",
    "\n",
    "For player $2$:\n",
    "$$\n",
    "B = \n",
    "\\begin{bmatrix} \n",
    "-1 & 1 \\\\\n",
    "1 & -1\\\\\n",
    "\\end{bmatrix} \n",
    "$$\n",
    "\n",
    "Player $1$ wants to play *the same action* as player 2, who however wants to play *the opposite action* as player $1$. What can the agents do? How can they try and adapt their strategies w.r.t. their opponent?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RL with Natural Gradients and Matrix Games\n",
    "\n",
    "One possible way to approach the problem is to construct *two* Reinforcement Learning agents, which will play against each other for many and many repetitions, and update their policies using a Policy Gradient method.\n",
    " \n",
    "In particular, there is an important special choice of method, which is the one using the Natural Gradient and soft-max policies.\n",
    "\n",
    "Recall that in this case we gave for each action a parameter $\\theta$ such that:\n",
    "$$\n",
    "\\pi_{\\theta}(a) = \\frac{e^{\\theta_{a}}}{\\sum_{a'} e^{\\theta_{a'}}}\n",
    "$$\n",
    "\n",
    "It turns out (see Lecture 18) that the Natural Gradient for a Matrix Games, we have that the *gradient of the expected payoff* w.r.t. the parameters $\\theta$ is:\n",
    "\n",
    "$$\n",
    "\\tilde{\\nabla}_{\\theta_a^{(1)}}G^{(1)} = \\sum_{a_2} A_{a \\, a_2} \\pi^{(2)}(a_2)\n",
    "$$\n",
    "\n",
    "What is the righten sideof the equation?\n",
    "It is exactly the payoff that agent gets on average when it plays action $a$, and player $2$ plays its strategy $\\pi^{(2)}(1), \\pi^{(2)}(2)$. \n",
    "\n",
    "\n",
    "This means that at each time the action with higher payoff gets exponentially ($\\sim e^{\\theta_a^{(1)}}$!) more probable. (But since also the strategy of player $2$ changes, that same action can become less favorable in the future...).\n",
    "\n",
    "The Natural Gradient algorithm for Adversarial Bandits is called also EXP3 *(EXPonential-weight algorithm for EXPloration and EXPloitation)*, and as such has been well studied for its properties of convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "class ActorCritic_NaturalGradient():\n",
    "    def __init__(self, \n",
    "                 space_size, \n",
    "                 action_size, \n",
    "                 gamma=1, \n",
    "                 lr_v = 0.01,\n",
    "                 lr_a = 0.01,\n",
    "                 eta = 0.01,\n",
    "                 start_prob = np.array([])):\n",
    "        \"\"\"\n",
    "        Calculates optimal policy using in-policy Temporal Difference control\n",
    "        Approximate V-value for states S!\n",
    "        \"\"\"        \n",
    "        # the discount factor\n",
    "        self.gamma = gamma\n",
    "        # size of system\n",
    "        self.space_size = space_size\n",
    "        # action size\n",
    "        self.action_size = action_size\n",
    "        \n",
    "        # the learning rate for value\n",
    "        self.lr_v = lr_v\n",
    "        # the learning rate for actor\n",
    "        self.lr_a = lr_a\n",
    "        \n",
    "        # to diminuish variance\n",
    "        self.eta = 0.01\n",
    "        \n",
    "        # Stores the Value Approximation weights\n",
    "        self.w = np.zeros((*self.space_size,))\n",
    "        # Stores the Policy parametrization\n",
    "        self.Theta = np.zeros( (*self.space_size, self.action_size) )\n",
    "    \n",
    "        if not start_prob.size == 0:\n",
    "            assert start_prob.shape == (*self.space_size, self.action_size), 'Starting policy does not match game dimensions.'\n",
    "            self.Theta = np.log(start_prob)\n",
    "    \n",
    "    # -------------------   \n",
    "    def single_step_update(self, s, a, r, new_s, done):\n",
    "        \"\"\"\n",
    "        Uses a single step to update the values, using Temporal Difference for V values.\n",
    "        Employs the EXPERIENCED action in the new state  <- Q(S_new, A_new).\n",
    "        \"\"\"\n",
    "        \n",
    "        # ---------------------\n",
    "        # CRITIC UPDATE -------\n",
    "        # ---------------------\n",
    "        \n",
    "        if done:\n",
    "            # -----------------------\n",
    "            delta = (r + 0 - self.w[(*s,)])\n",
    "            \n",
    "        else:\n",
    "            # --------------------------\n",
    "            delta = (r + \n",
    "                      self.gamma * self.w[(*new_s,)]\n",
    "                                 - self.w[(*s,)])\n",
    "            \n",
    "        # --------------------\n",
    "        self.w[(*s,)] += self.lr_v * delta \n",
    "        \n",
    "        # -----------------------------------------------\n",
    "        # Now Actor update with Natural Gradient --------\n",
    "        # -----------------------------------------------\n",
    "        policy = self.get_policy(s)\n",
    "        \n",
    "        # Natural Gradient: non-zero only for the action which has been selected!\n",
    "        self.Theta[(*s, a) ] += self.lr_a * delta / (policy[a] + 0.001)\n",
    "\n",
    "        \n",
    "    # ---------------------\n",
    "    def get_action(self, s):\n",
    "        \"\"\"\n",
    "        Chooses action at random using policy from current parameters Theta.\n",
    "        \"\"\"\n",
    "        # Prob(a|s) = exp(theta_a) / (sum_a' exp(theta_a') \n",
    "\n",
    "        # Common shift has no influence on relative probabilities\n",
    "        # But very helpful: Otherwise it can explode! \n",
    "        log_prob = self.Theta[(*s,)] - np.mean(self.Theta[(*s,)])\n",
    "        \n",
    "        # Clip needed to avoid NaN.\n",
    "        log_prob = np.clip(log_prob, -20, 20)\n",
    "        \n",
    "        # P(a_i) = exp(theta_i) / sum_j exp(theta_j) \n",
    "        prob_actions = np.exp(log_prob)\n",
    "        prob_actions = prob_actions / np.sum(prob_actions)\n",
    "        \n",
    "        # take one action from the array of actions with the probabilities as defined above.\n",
    "        a = np.random.choice(self.action_size, p=prob_actions)\n",
    "        return a \n",
    "    \n",
    "        # ---------------------\n",
    "    def get_policy(self, s):\n",
    "        \"\"\"\n",
    "        Returns policy from current parameters Theta.\n",
    "        \"\"\"\n",
    "        # Prob(a|s) = exp(theta_a) / (sum_a' exp(theta_a') \n",
    "        \n",
    "        # Common shift has no influence on relative probabilities\n",
    "        # But very helpful: Otherwise it can explode! \n",
    "        log_prob = self.Theta[(*s,)] - np.mean(self.Theta[(*s,)])\n",
    "        \n",
    "        # Clip needed to avoid NaN.\n",
    "        log_prob = np.clip(log_prob, -20, 20)\n",
    "        \n",
    "        prob_actions = np.exp(log_prob)\n",
    "        prob_actions = prob_actions / np.sum(prob_actions)\n",
    "            \n",
    "        return prob_actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evolution of policies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 10000 20000 30000 40000 50000 60000 70000 80000 90000 [-0.05987  0.05987]\n"
     ]
    }
   ],
   "source": [
    "# ActorCritic with Natural Gradient: Bootstrapping to solve CartPole (with state aggregation).\n",
    "\n",
    "n_episodes = 100000\n",
    "\n",
    "gamma = 1\n",
    "\n",
    "# Initialize \n",
    "lr_v_0 = 0.1\n",
    "lr_a_0 = 0.0001\n",
    "\n",
    "lr_v = lr_v_0\n",
    "lr_a = lr_a_0\n",
    "\n",
    "PayoffMatrix1 = np.array([[1, -1],\n",
    "                          [-1, 1]]\n",
    "                        )\n",
    "\n",
    "PayoffMatrix2 = np.array([[-1, 1],\n",
    "                          [1, -1]]\n",
    "                        )\n",
    "\n",
    "starting_prob_1 = np.array([[0.15, 0.85]])\n",
    "starting_prob_2 = np.array([[0.85, 0.15]])\n",
    "\n",
    "# Initialize algorithm\n",
    "Player1 = ActorCritic_NaturalGradient(space_size=[1], \n",
    "                      action_size=PayoffMatrix1.shape[0],\n",
    "                      gamma=gamma, \n",
    "                      lr_v=lr_v,\n",
    "                      lr_a=lr_a,\n",
    "                      start_prob=starting_prob_1)\n",
    "\n",
    "# Initialize algorithm\n",
    "Player2 = ActorCritic_NaturalGradient(space_size=[1], \n",
    "                      action_size=PayoffMatrix2.shape[0],\n",
    "                      gamma=gamma, \n",
    "                      lr_v=lr_v,\n",
    "                      lr_a=lr_a,\n",
    "                      start_prob=starting_prob_2)\n",
    "\n",
    "traj_policy_1 = np.empty([0])\n",
    "traj_policy_2 = np.empty([0])\n",
    "\n",
    "av_rew = np.zeros(2)\n",
    "# RUN OVER EPISODES\n",
    "done = False\n",
    "for i in range(n_episodes):\n",
    "    \n",
    "    if (i%10000==0):\n",
    "        print(i, end=' ')\n",
    "\n",
    "    # There is only one state!\n",
    "    s = [0]\n",
    "\n",
    "    # Select action with Soft-max\n",
    "    a_1 = Player1.get_action(s)\n",
    "    a_2 = Player2.get_action(s)\n",
    "\n",
    "    r_1 = PayoffMatrix1[a_1, a_2]\n",
    "    r_2 = PayoffMatrix2[a_1, a_2]\n",
    "\n",
    "    if i > n_episodes*0.8:\n",
    "        av_rew += [r_1, r_2]\n",
    "    \n",
    "    # SINGLE STEP UPDATE    \n",
    "    Player1.single_step_update(s, a_1, r_1, s, done)\n",
    "    Player2.single_step_update(s, a_2, r_2, s, done)        \n",
    "\n",
    "    traj_policy_1 = np.append(traj_policy_1, Player1.get_policy([0])[0])\n",
    "    traj_policy_2 = np.append(traj_policy_2, Player2.get_policy([0])[0])\n",
    "      \n",
    "print(av_rew/n_episodes)\n",
    "\n",
    "np.savetxt('FirstTry.txt', np.append(traj_policy_1.reshape(-1,1), traj_policy_2.reshape(-1,1), axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replicator dynamics\n",
    "\n",
    "This kind of evolution of probabilities is known in Evolutionary Game Theory as Replicator Dynamics. \n",
    "\n",
    "Replicator Dynamics is a mathematical model of growth that is used to study competing strategies/populations. In RD the population is divided into different types $x_i$, and each class has a *fitness* (how good are they) $f(x_i)$. The average *fitness* of the population is $\\phi(x)$.\n",
    "\n",
    "\n",
    "**In RD, the rate of growth of each class is proportional to how much better their indivual fitness is in respect to the average**.\n",
    "\n",
    "$$\n",
    "\\dot{x_i} = x_i (f(x_i) - \\phi(x))\n",
    "$$\n",
    "\n",
    "In our case the types of population are the different actions and their probability. The fitness is how well they do against the opponent!\n",
    "\n",
    "$$\n",
    "x_i \\rightarrow \\pi^{(1)}(i) \\\\\n",
    "f(x_i) \\rightarrow \\sum_k A_{i k} \\pi^{(2)}(k)\n",
    "$$\n",
    "The average fitness is just:\n",
    "\n",
    "$$\n",
    "\\phi(x) = \\sum_{j, k} A_{j k} \\pi^{(1)}(j) \\pi^{(2)}(k)\n",
    "$$\n",
    "\n",
    "So we have that doing Multi Agent Reinforcement Learning with Natural Gradient in Matrix Games is equivalent to let the policies \"evolve\" with the Replicator Dynamics. We don't really need an algorithm, and can directly solve the differential equations for the policies.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The solution for REPLICATOR DYNAMICS\n",
    "\n",
    "from scipy.integrate import solve_ivp as solve\n",
    "\n",
    "p1_0 = 0.65\n",
    "p2_0 = 0.65\n",
    "\n",
    "# Payoff matrix for player 1\n",
    "A = np.array([[-1, 1],\n",
    "              [1, -1]])\n",
    "\n",
    "# Payoff matrix for player 2\n",
    "B = np.array([[1, -1],\n",
    "              [-1, 1]])\n",
    "             \n",
    "    \n",
    "# Scipy want a function   fun(t, p) to integrate numerically:\n",
    "# d(p)/dt = fun(t, p)\n",
    "def wrapper_payoff(A, B):\n",
    "    def payoff(t, p):\n",
    "        \n",
    "        p1 = np.array([p[0], 1-p[0]])\n",
    "        p2 = np.array([p[1], 1-p[1]])\n",
    "        \n",
    "        # Matrix multiplication between A and pi^(2)\n",
    "        # sum_j A_1j p^(2)(j) = FITNESS OF ACTION 1 for player 1\n",
    "        # sum_j A_2j p^(2)(j) = FITNESS OF ACTION 2 for player 1\n",
    "        A_x_p2 = np.matmul(A, p2)\n",
    "        # Matrix multiplication between A and pi^(1)\n",
    "        # sum_j B_j1 p^(1)(j) = FITNESS OF ACTION 1 for player 2\n",
    "        # sum_j B_j1 p^(1)(j) = FITNESS OF ACTION 2 for player 2\n",
    "        B_x_p1 = np.matmul(B, p1)\n",
    "        \n",
    "        # Average fitness for player 1\n",
    "        p1T_A_x_p2 = np.dot(p1, A_x_p2)\n",
    "        # Average fitness for player 2                   \n",
    "        p2T_B_x_p1 = np.dot(p2, B_x_p1)\n",
    "\n",
    "        expected_payoff = np.zeros(2)\n",
    "                   \n",
    "        # REPLICATOR FORMULA for differential update!\n",
    "        expected_payoff[0] = p1[0] * (A_x_p2[0]  - p1T_A_x_p2)\n",
    "        expected_payoff[1] = p2[0] * (B_x_p1[0]  - p2T_B_x_p1)\n",
    "        return expected_payoff\n",
    "    # this returns the above function.\n",
    "    return payoff\n",
    "\n",
    "sol = solve(wrapper_payoff(A,np.transpose(B)), (0,100), np.array([p1_0, p2_0]),first_step=0.01, max_step=0.05)\n",
    "np.savetxt('ODE_solution.txt', np.transpose(sol.y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(\n",
    "### Social Dilemmas\n",
    "\n",
    "Prisoner's Dilemma: https://en.wikipedia.org/wiki/Prisoner%27s_dilemma \n",
    "\n",
    "Two possibilities: Accuse other / Do not accuse other.\n",
    "\n",
    "$$\n",
    "\\text{Years in Prison} = \\begin{bmatrix} \n",
    "-6, -6 & 0, -7 \\\\\n",
    "-7, 0 & -1, -1\\\\\n",
    "\\end{bmatrix} \n",
    "$$\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic reward\n",
    "\n",
    "So far we have dealt with deterministic payoffs: Given the two actions $a_1$ and $a_2$ the results is perfectly determined. Note that this does not need to be true, we could also have that *on average* the payoffs could be that of the matrix, but each single game could have a stochastic payoff. \n",
    "\n",
    "\n",
    "While for sure an human would be much confused (try play football if sometimes you have to score in one goal and sometimes in the other and you never know where...) the algorithm works even in that case!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
