{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> CartPole, Deep Q-learning guided exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a neural network as a quality function approximator\n",
    "\n",
    "The contiuous state space of the CartPole environemnt does not allow for a tabular learning of the quality function.\n",
    "A typical approach in such cases is to use a neural network as a funcion approximator for the quality function, which has the state as input (4 input nodes, corresponding to the 4 dimensions of the state space), and returns the quality of each action as output (2 output nodes, i.e. push left or push right).\n",
    "\n",
    "<center> $Q(\\; \\vec{s} \\;, \\text{push left} \\; | \\;\\vec{w}) = \\text{first output node of the neural network with weights } \\vec{w} \\text{ and input } \\vec{s}$\n",
    "    \n",
    "<center> $Q(\\; \\vec{s} \\;, \\text{push right} \\; | \\;\\vec{w}) = \\text{second output node of the neural network with weights } \\vec{w} \\text{ and input } \\vec{s}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General idea of deep-Q-learning\n",
    "\n",
    "As for a generic tabular reinforcement learning algorithm, the point is to find a good approximation of the quality function, such that I can choose my policy by reliably knowing what I am going to gain with all the actions that I have.\n",
    "\n",
    "For example, in the case of Q-learning, given an *experience* of starting state $s$, action taken $a$, reward obtained $r$, and new state reached $s'$, the quality associated to $s$ and $a$ is \"corrected\" (up to a factor given by the learning rate) by the following temporal difference error:\n",
    "\n",
    "$$\n",
    "r + \\gamma \\max_{b} Q(s', b) - Q(s, a) = \\hat{Q}(s, a) - Q(s, a)\n",
    "$$\n",
    "\n",
    "where $\\hat{Q}(a,s)$ is the target of the quality function at that step.\n",
    "\n",
    "In the case of a quality function approximator, e.g. our NN, the algorithm can no longer directly change the specific number $Q(s, a)$, but it has to change the parameters/weights of the approximator affecting the shape of $Q(s,a| \\vec{w})$.\n",
    "A possible choice is to define the following mean square error as a loss function, for a given sample of experiences $\\lbrace (s, a, r, s') \\rbrace = \\lbrace e \\rbrace$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "L(\\vec{w}) = & \\frac{1}{2} \\sum_{\\lbrace e \\rbrace} (\\hat{Q}(s, a | \\vec{w}) - Q(s, a | \\vec{w}))^2\n",
    "\\\\\n",
    "& \\frac{1}{2} \\sum_{\\lbrace e \\rbrace} (r + \\gamma \\max_{b} Q(s', b) - Q(s, a | \\vec{w}))^2\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "which uses the same off-policy estimate of the Q-learning as a target of the quality function.\n",
    "Hopefully, by accumulating several experiences $(s, a, r, s')$, and minimizing the loss function with respect to the parameters $\\vec{w}$, the \"true\" quality function will be well approximated, and the final best policy will reach a high return."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we define a simple two-layer network that can do the job of approximating the quality function of the cart-pole problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "\n",
    "\n",
    "def build_NN(input_dim, output_dim, adam_learning_rate):\n",
    "    \"\"\"\n",
    "    Bulding the neural network as a keras model. The network has specified number of input and output nodes.\n",
    "    Is has 2 hidden layers composed of 100 units. All the layers are dense.\n",
    "    Activation functions are relu for the hidden layers , and linear for the output layer.\n",
    "    The loss function that the network tries to minimize is a mean squared error.\n",
    "    The weights are updated with the ADAM optimizer (an improvement of GD which \n",
    "    also considers information about the second moment of the error to adaptively change the \n",
    "    learning rate).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the four layers of the neural network\n",
    "    input_layer = Input(shape=(input_dim,))\n",
    "    h1 = Dense(100, activation=\"relu\")(input_layer)\n",
    "    h2 = Dense(100, activation=\"relu\")(h1)        \n",
    "    output_layer = Dense(output_dim, activation=\"linear\")(h2)\n",
    "    \n",
    "    # Putting the layers together and specifying the loss function and the GD algorithm.\n",
    "    model = Model(input_layer, output_layer) \n",
    "    model.compile(loss=\"mse\", optimizer=Adam(lr=adam_learning_rate))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two methods that you need to train the network in our reinforcement learning setting are *predict* and *fit*. For the documentation see: https://keras.io/models/model/.\n",
    "They are used within the \"brain\" presented below.\n",
    "\n",
    "*Predict(state)* returns the output associated to the input, i.e. the quality of the two actions given the state, according to the current configuration of weights.\n",
    "\n",
    "*Fit(state, $\\hat{Q}$)* trains the network by performing one step of minimization of the loss function (the algorithm used for minimization is specified in the keras model construction). This requires the,  which are the \"inputs\" of the NN, and the estimates of the quality function from that state, i.e. the \"labels\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the brain that will perform the training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following class will perform the crucial steps during the learning cycle. \n",
    "\n",
    "An important trick used here, that crucially imporove the performance, is to use a memory of experiences, $\\lbrace (s, a, r, s') \\rbrace$, and perform a training (i.e. a minimization step of the loss function) over samples of these experiences.\n",
    "In the description below, the exact procedure is better explained.\n",
    "\n",
    "Summary of the methods of the brain:\n",
    "- *act(state, $\\epsilon$)*: returns the action given the state and the episode. There is performed with epsilon-greedy exploration: with probability $\\epsilon$ the action is chosen at random, otherwise the argmax of the estimated quality is selected.\n",
    "- *remember(state, action, next_state, reward)*: add to the memory the *experience*. The memory has a size $\\text{mem_size}$: when more experiences are obtained the first ones are removed. \n",
    "- *train()*:\n",
    "> - sample a mini-batch of experiences uniformly from the memory. Minibatch size: $\\text{batch_size}$\n",
    "> - for each experience compute the new Q-learning estimate: $\\hat{Q} = r + \\gamma max_{a'} Q(s',a',\\textbf{w})$ (you need the *Predict* method of the keras NN to get all the $Q$s). Note that if the state $s$ is terminal, the estimate is just the obtained reward. \n",
    "> - SGD step to adjust the $\\textbf{w}$ with the minimization of $L(\\textbf{w}) = \\frac{1}{2}(\\hat{Q} - Q)^2$ over the sampled expereinces (these uses the *Fit* method of keras model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "\n",
    "class DQN_Brain:\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, learning_rate=.005, mem_size=5000, batch_size=64, gamma=1.):\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.memory = deque(maxlen=mem_size) # Define our experience replay bucket as a deque with size mem_size.\n",
    "        self.model = build_NN(input_dim, output_dim, learning_rate)\n",
    "        \n",
    "        \n",
    "    def act(self, state, explore_p):\n",
    "        # With probability explore_p, randomly pick an action\n",
    "        if explore_p > np.random.rand():\n",
    "            return np.random.randint(self.output_dim)\n",
    "        # Otherwise, find the action that should maximize future rewards according to our current Q-function policy.\n",
    "        else:\n",
    "            return np.argmax(self.model.predict(np.array([state])))\n",
    "            \n",
    "        \n",
    "    def remember(self, state, action, next_state, reward):\n",
    "        # Create a blank state. Serves as next_state if this was the last experience tuple before the epoch ended.\n",
    "        terminal_state = np.array([None]*self.input_dim) \n",
    "        # Add experience tuple to bucket. Bucket is a deque, so older tuple falls out on overflow.\n",
    "        self.memory.append((state, action, terminal_state if next_state is None else next_state, reward))\n",
    "        \n",
    "        \n",
    "    def train(self):\n",
    "\n",
    "        # Only conduct a replay if we have enough experience to sample from.\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        # Pick random indices from the bucket without replacement. batch_size determines number of samples.\n",
    "        idx = np.random.choice(len(self.memory), size=self.batch_size, replace=False)\n",
    "        minibatch = np.array(self.memory)[idx]\n",
    "\n",
    "        # Extract the experience from our sample\n",
    "        states = np.array(list(minibatch[:,0]))\n",
    "        actions = minibatch[:,1]\n",
    "        rewards = np.array(minibatch[:,3])\n",
    "        next_states = np.array(list(minibatch[:,2]))\n",
    "        \n",
    "        # Compute a new estimate for each Q-value\n",
    "        estimate = rewards + self.gamma * np.amax(self.model.predict(next_states), axis=1)\n",
    "\n",
    "        # Get the network's current Q-value predictions for the states in this sample.\n",
    "        predictions = self.model.predict(states)\n",
    "        # Update the network's predictions with the new predictions we have.\n",
    "        for i in range(len(predictions)):\n",
    "            # Flag states as terminal (the last state before a epoch ended).\n",
    "            terminal_state = (next_states[i] == np.array([None]*self.input_dim)).all()\n",
    "            # Update each state's Q-value prediction with our new estimate.\n",
    "            # Terminal states have no future, so set their Q-value to their immediate reward.\n",
    "            predictions[i][actions[i]] = rewards[i] if terminal_state else estimate[i]\n",
    "\n",
    "        # Propagate the new predictions through our network.\n",
    "        self.model.fit(states, predictions, epochs=1, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pseudocode\n",
    "\n",
    "Now you have all the ingredeints to write down the learning cycle that can solve the cart-pole problem.\n",
    "The pseudocode below provides a possible implementation of the algorithm.\n",
    "\n",
    "- construct the brain object with all the desired hyperparameters. The input and output dimensions have to corresponed to the state space dimension and the number of possible actions.\n",
    "- for episode until a given rule is satisfied (e.g. run the cycle for a given number of episodes):\n",
    "> - reset the environment to the starting state: *starting_state = env.reset()*\n",
    "> - for each step in the episode (until any terminal state has been reached)   \n",
    "> > - choose an exploration probability according to a given epsilon-greedy scheduling\n",
    "> > - select an explorative action or a greedy one $a_t$ according to the quality estimate of the NN. You just need to call the *act(state, $\\epsilon$)* method of the brain.\n",
    "> > - execute the action $a_t$ and observe reward $r_{t+1}$, the new state $s_{t+1}$, and whether a terminal state is reached: *new_state, reward, done, _ = env.step(action)*\n",
    "> > - if a terminal state is reached (*done = True*) break the cycle.\n",
    "> > - save the experience $e_t = \\{s_t,a_t,r_{t+1},s_{t+1}\\}$ in a memory: *brain.remember(state, action, next_state, reward)*\n",
    "> > - train the brain according to the stored experience: *brain.train()*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "As often happens in machine learning, you have a lot of freedom in choosing the hyperparameters and adding possible tricks to imporve the performances.\n",
    "The right way to choose those parameters strongly depends on your specific problem, and usually there are no general and correct recipes.\n",
    "\n",
    "Here we just list the crucial parameters and rules of these algorithm.\n",
    "\n",
    "- **Discount factor**. It is suggested to choose a value close to 1. In this way, the return from the step $t$ is approximatley the number of steps from $t$ to the termination of the episode.\n",
    "\n",
    "- **Learning rate**. A constant rate for the adam optimizer can do the job, but maybe the efficiency can be increased if the learning rate decreases with time.\n",
    "\n",
    "- **Batch size**. Number of experiences sampled from the memory over which the algorithm is trained at each step.\n",
    "\n",
    "- **Exploration rate**. Here you can employ an epsilon-greedy strategy. It is suggested to have a decreasing probability of exploration with the number of steps or episodes.\n",
    "\n",
    "- **Max steps per episode**. You can tune this with *env.\\_max\\_episode\\_steps*. You can also consider to increase this variable as the training advances.\n",
    "\n",
    "- **Stopping rule**. You can stop the training after a given number of steps / episodes, but maybe it is better to define something that quantifies if the algorithm has learned (e.g. stop if the last 15 episodes have a reward equal to *\\_max\\_episode\\_steps*).\n",
    "\n",
    "- **Neural network**. You can also play with the network, e.g. modifying the units in the hidden layers, or the number of hidden layers...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write your algorithm here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have created a cart pole that is able to balance the pole for a number of episode equal to *env.\\_max\\_episode\\_steps*.\n",
    "How does it behave for longer times?\n",
    "\n",
    "Test it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is for testing a trained model. \n",
    "# NOTE: the second argument is the keras model, not the brain object!\n",
    "def Test(env, NN_keras_model, max_n_steps, render=True):\n",
    "    \n",
    "    env._max_episode_steps = max_n_steps\n",
    "    state = env.reset()\n",
    "    ep_reward = 0\n",
    "    \n",
    "    while True: # Cycle over the episode steps\n",
    "        if render:\n",
    "            env.render()\n",
    "        action = np.argmax(NN_keras_model.predict(np.array([state]))) # Get action without exploration\n",
    "        next_state, reward, done, _ = env.step(action) # Take action\n",
    "        ep_reward += reward # Accumulate reward\n",
    "        state = next_state # Advance state\n",
    "        if done: # Episode is completed -- failure or max number of steps reached (success)\n",
    "            print(\"Total reward: {}\".format(ep_reward))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing an already trained network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "\n",
    "def load_NN(directory, name):\n",
    "    # load json and create model\n",
    "    json_file = open(directory + name+ '.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    loaded_model.load_weights(directory + name + '.h5')\n",
    "    print(\"Loaded model from disk\")\n",
    "    return loaded_model\n",
    "\n",
    "trained_model = load_NN('', 'trained_keras_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "Test(env, trained_model, 10000, render=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
